---
title: "P8105_HW2_bjl2150"
author: "Briana Lettsome"
date: "October 5th, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

# Problem 1

### Importing NYC Transit csv file
```{r}
# Reading in the NYC transit csv file plus cleaning the data
nyc_transit = read_csv(
  file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names()

# Removing the not requested variables
nyc_transit_clean = select(nyc_transit, -exit_only, -staffing, -staff_hours, 
                           -ada_notes,-free_crossover, -north_south_street, 
                           -east_west_street, -corner, -entrance_latitude, 
                           -entrance_longitude, -station_location, 
                           -entrance_location ) %>% 
  mutate(entry = recode(entry, YES = TRUE, NO = FALSE))

# Obtaining the dimension (rows X columns) of the datafile NYC_Transit_clean
dim(nyc_transit_clean)
```

* The NYC dataset contains information about all of the subway lines used throughout 
NYC. Some of the variables provided include, station name, the station line, the
routes that the trains take as well as whether the trains are ADA compliant. 
 * After importing the csv file, the steps undertaken to clean the dataset were
 cleaning the column names (using the 'janitor' function), removing the unrequested 
 variables (using the 'select' function), and coercing the column variable 'entry'
into a character variable (the functions 'mutate' and 'recode' were used here).


```{r}
# Question 1: Distinct function

 distinct(nyc_transit_clean, station_name, line)
```
 
There are 465 distinct stations.

```{r}
# Question 2: ADA compliance

filter(nyc_transit_clean, ada == TRUE) %>%
distinct(station_name, line, ada = TRUE)
```
There are 84 stations that are ADA compliant.

```{r}
# Question 3: Proportion




filter(nyc_transit_clean, vending == "NO") %>%
nrow()

filter(nyc_transit_clean, entry == "TRUE", vending == "NO") %>%
nrow()
```

```{r}
# Gathering route columns to make a tidier dataset

nyc_transit_tidy =
  gather(nyc_transit_clean, key = "route_number", value = "route_name", route1:route11) 

## Determining distinct stations that serve A train

filter(nyc_transit_tidy, route_name == "A") %>%
distinct(station_name, line, route_name = "A")
```

There are 60 distinct stations that served the A train.

```{r}
# Determining ADA compliance

filter(nyc_transit_tidy, route_name == "A", ada == TRUE) %>%
distinct(station_name, line, ada = TRUE)
```
There are 17 distinct stations that are ADA compliant.


# Problem 2

## Use of Mr. Trash Wheel excel dataset
```{r}
## Part 2 A

library(readxl)

## Importing and cleaning Mr. Trash Wheel excel file

trash_wheel = read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", 
              sheet = "Mr. Trash Wheel")


trash_wheel = read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", 
              sheet = "Mr. Trash Wheel", range = "A2:N336") %>% 
              janitor::clean_names() %>% 
              filter(dumpster != "N/A") %>% 
mutate(sports_balls = as.integer(sports_balls)) 
```

```{r}
## Part 2B

## Reading in Trash Wheel, selecting in sheet "2016 Precipiation", and cleaning
library(readxl)

precip_16 = read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", 
              sheet = "2016 Precipitation", range = "A2:B14") %>%
 janitor::clean_names() %>%
  mutate(year = 2016)


precip_17 = read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", 
              sheet = "2017 Precipitation", range = "A2:B14") %>%
 janitor::clean_names() %>%
  mutate(year = 2017)

precipitation = bind_rows(precip_16, precip_17) %>%
   mutate(month = month.name[month])


# Calculating the total precipiation in 2017

total_precip = filter(precipitation, year == 2017)

summarize(total_precip, total = sum(total))
 
 # Calcuating the median number of sports balls in dumpster in 2016

sports_2016 = filter(trash_wheel, year == 2016)

median(sports_2016$sports_balls)
  

```
Regarding these datasets, the Mr. Trash Wheel excel file was imported and the sheet,
specifically focused on Mr. Trash Wheel, was selected for analysis. There were many
variables retained, including dumpster number, weight of garbage in tons and numbers
of sports balls and plastic bottles. The number ofobservations in the the dataset 
trash_wheel was calculated to be `r dim(trash_wheel)`. They key variables of 
interest were 

 
There are `r dim(precipitation)` observations in the precipiation dataset. This 
dataset was created by merging the precip_16 and precip_17 datasets from the Mr. 
Trash Wheel excel file. The key variables in this particular data set are the
months and years of precipiation as well as the toal amount of precipation 
`r summarize(total_precip, total = sum(total))`. The median number of spors balls 
in a dumpster is `r median(sports_2016$sports_balls)`.


```{r}

```

# Problem 3

## Importing, cleaning, filtering and tidying brfss_smart2010 dataset

```{r}
# install.packages("devtools")

devtools::install_github("p8105/p8105.datasets")

library(p8105.datasets)
library(janitor)

data(brfss_smart2010)
  
brfss_smart2010 = brfss_smart2010 %>% 
  clean_names() %>%
  filter(topic == "Overall Health") %>%
select(-class, -topic, -question, -sample_size, 
       -(confidence_limit_low:geo_location))

brfss_spread = spread(brfss_smart2010, key = response, value = data_value) %>%
  janitor::clean_names() %>%
  mutate(proportion_reponse = (excellent + very_good))

# Determining how many unique locations in dataset

select(brfss_spread, locationdesc) %>%
sapply(function(locationdesc) length(unique(locationdesc)))

# Every represente state

select(brfss_spread, locationabbr) %>%
  distinct(locationabbr) %>%
  count()

count(brfss_spread, vars = locationabbr) %>% View


# Filtering year = 2002
brfss_spread_2 = filter(brfss_spread, year == 2002)

  
# Finding the median of the excellent responses
median(brfss_spread_2$excellent, na.rm = TRUE)
```

## Answering the Problem 3 questions

* There are 404 unique locations included within this dataset.

* Every state is represented. There are total variables within the 'locationsabbr'
  column indicating that all  states are represented plus an additional state, which 
  is Washington, D.C.

* The state that is observed the most is New Jersey with 146 observations.
  
 
* `r median(brfss_spread_2$excellent, na.rm = TRUE)` is the median of the "Excellent"
    response values in 2002. 


### Histogram of "Excellent" responses in 2002
```{r}
ggplot(brfss_spread_2, aes(x = excellent)) + geom_histogram()
```

### Scatterplot of Queens and New York Counties
```{r}

# Selecting of specific columns
brfss_scatter = select(brfss_spread, excellent, locationdesc, year)

# Specifiying which vaiables in 'locationdesc' column
target <- c("NY - New York County", "NY - Queens County") 
  
# Creating the dataframe to make scatterplot with above specifications
brfss_plot = filter(brfss_scatter, locationdesc %in% target) 

# Making of scatterplot
ggplot(brfss_plot, aes(x = excellent, y = year)) + 
  geom_point(aes(color = locationdesc))

```
